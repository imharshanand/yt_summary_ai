{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dae92f-cdf6-4339-b599-1ce15f0f3991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is valid.\n",
      "Channel ID: UCXuqSBlHAE6Xw-yeJA0Tunw\n"
     ]
    }
   ],
   "source": [
    "# Check YouTube API and and get the channel ID with a video link.\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "def check_youtube_api_key(api_key):\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q='test',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        print(\"API Key is valid.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"API Key is invalid: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_channel_id_from_video(api_key, video_url):\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid video URL.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        request = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response and len(response['items']) > 0:\n",
    "            channel_id = response['items'][0]['snippet']['channelId']\n",
    "            print(f\"Channel ID: {channel_id}\")\n",
    "            return channel_id\n",
    "        else:\n",
    "            print(\"No video found with the provided ID.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving channel ID: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_video_id(url):\n",
    "    # Regular expression to extract the video ID from a YouTube URL\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = 'AIzaSyDeGQSEB_zIzpk3nuetwYIwSuEWvug61nQ'\n",
    "    VIDEO_URL = 'https://www.youtube.com/watch?v=aVUxxn53mBE'  # Replace with your video URL\n",
    "\n",
    "    if check_youtube_api_key(API_KEY):\n",
    "        get_channel_id_from_video(API_KEY, VIDEO_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9bc74c-4711-4873-bdf5-f4138764c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/llm_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 3060 Ti\n",
      "Device being used: cuda\n",
      "Python Version: 3.8.19 (default, Mar 20 2024, 19:58:24) \n",
      "[GCC 11.2.0]\n",
      "Transformers Version: 4.41.2\n",
      "Torch Version: 2.3.1+cu121\n",
      "\n",
      "Model Architecture:\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import os\n",
    "\n",
    "# Directory where the model is saved\n",
    "model_dir = \"./model\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Function to print environment information\n",
    "def print_environment_info():\n",
    "    # Check for CUDA\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0) if cuda_available else \"N/A\"\n",
    "    \n",
    "    print(\"Environment Information:\")\n",
    "    print(f\"CUDA Available: {cuda_available}\")\n",
    "    print(f\"CUDA Device Count: {cuda_device_count}\")\n",
    "    print(f\"CUDA Device Name: {cuda_device_name}\")\n",
    "    print(f\"Device being used: {device}\")\n",
    "\n",
    "    # Print Python version\n",
    "    import sys\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "    # Print installed versions of key libraries\n",
    "    import transformers\n",
    "    print(f\"Transformers Version: {transformers.__version__}\")\n",
    "    print(f\"Torch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "# Print environment information\n",
    "print_environment_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba04e2-309b-46a2-accd-db183d4b65be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
